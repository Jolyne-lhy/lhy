{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1362ec",
   "metadata": {},
   "source": [
    "# Leak localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a0105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import networkx as nx\n",
    "import epynet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2a3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Importing custom libraries\n",
    "# source: https://github.com/GardarGardarsson/GSP_for_Leak_Detection/tree/864a99041af6331375946a8fad3a71c633108eb8\n",
    "# --------------------------\n",
    "\n",
    "# Import a custom tool for converting EPANET .inp files to networkx graphs\n",
    "from ep_utils.epanet_loader import get_nx_graph\n",
    "\n",
    "# Function for visualisationa\n",
    "from ep_utils.visualisation import visualise\n",
    "\n",
    "# EPANET simulator, used to generate nodal pressures from the nominal model\n",
    "from ep_utils.epanet_simulator import epanetSimulator\n",
    "\n",
    "# SCADA timeseries dataloader\n",
    "from ep_utils.data_loader import battledimLoader, dataCleaner, dataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5597826",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_wdn = './data/l-town-data/L-TOWN_Real.inp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c785c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the .inp file using the EPYNET library\n",
    "wdn = epynet.Network(path_to_wdn)\n",
    "\n",
    "# Solve hydraulic model for a single timestep\n",
    "wdn.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44355efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, _ = get_nx_graph(wdn, weight_mode='real_pipe_length', get_head=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7ac48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_by_pipe = {}\n",
    "\n",
    "for node in G:\n",
    "    for neighbour, connecting_edge in G[node].items():\n",
    "        if connecting_edge['name'] == 'SELF':\n",
    "            continue\n",
    "        else:\n",
    "            neighbours_by_pipe[connecting_edge['name']] = [node, neighbour]\n",
    "            \n",
    "pipe_by_neighbours = { str(neighbour_list) : pipe for pipe , neighbour_list in neighbours_by_pipe.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0abfa314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_to_true(predict, true):\n",
    "    distance = min(nx.shortest_path_length(G, source=predict, target=neighbours_by_pipe[true][0], weight='weight'),\n",
    "                   nx.shortest_path_length(G, source=predict, target=neighbours_by_pipe[true][1], weight='weight'))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e30df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './reconstruct_error/'\n",
    "files = natsorted(os.listdir(folder))\n",
    "df_dict = {}\n",
    "df_list = []\n",
    "for file in files:\n",
    "    if (file.split('.'))[-1] != 'csv':\n",
    "        continue\n",
    "    name, _ = os.path.splitext(file)\n",
    "    filePath = folder + file\n",
    "    df = pd.read_csv(filePath,index_col=0)\n",
    "    df_dict[name] = df\n",
    "    # 合并传感器\n",
    "    df_row_sums = pd.DataFrame(df.sum(axis=1))\n",
    "    df_row_sums.columns = [(file.split('.'))[0]]\n",
    "    df_list.append(df_row_sums)\n",
    "\n",
    "new_df = pd.concat(df_list,axis=1)\n",
    "sensors_error_df = pd.concat(list(df_dict.values()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66ab5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_dict = {\n",
    "'zone1': [['2019-03-17 06:00:00', 'p280'], ['2019-06-05 15:15:00', 'p277']],\n",
    "'zone2': [['2019-04-28 01:30:00', 'p331'], ['2019-11-06 19:40:00', 'p426']],\n",
    "'zone3': [['2019-04-06 01:00:00', 'p710'], ['2019-08-31 10:50:00', 'p721']],\n",
    "'zone4': [['2019-08-31 08:25:00', 'p800'], ['2019-12-07 11:15:00', 'p879']],\n",
    "'zone5': [['2019-01-25 03:35:00', 'p827'],\n",
    "['2019-06-14 00:25:00', 'p193'],\n",
    "['2019-11-06 21:10:00', 'p762']],\n",
    "'zone6': [['2019-07-10 15:05:00', 'p680']],\n",
    "'zone7': [['2019-03-28 02:00:00', 'p653'], ['2019-06-13 17:45:00', 'p142'], ['2019-12-07 11:45:00','p123']],\n",
    "'zone8': [['2019-01-16 13:20:00', 'p523'],\n",
    "['2019-04-06 09:35:00', 'p514'],\n",
    "['2019-08-31 10:15:00', 'p586']],\n",
    "'zone9': [['2019-11-30 23:00:00', 'p455']],\n",
    "'zone10': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c62ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_list = {\n",
    "\n",
    "'zone1' : ['n1','n4','n31'],\n",
    "\n",
    "'zone2' : ['n410','n429'],\n",
    "\n",
    "'zone3' : ['n342','n636','n644'],\n",
    "\n",
    "'zone4' : ['n296','n679','n722','n740'],\n",
    "\n",
    "'zone5' : ['n288','n726','n752','n769'],\n",
    "\n",
    "'zone6' : ['n215','n229'],\n",
    "\n",
    "'zone7' : ['n163','n188','n613'],\n",
    "\n",
    "'zone8' : ['n332','n495','n506','n549'],\n",
    "\n",
    "'zone9' : ['n105','n114','n469', 'n516'],\n",
    "\n",
    "'zone10' : ['n54','n415','n458', 'n519']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c1bb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    if norm_vector1 == 0 or norm_vector2 == 0:\n",
    "        return 0\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def localize(df, target):\n",
    "    max_similarity = -1\n",
    "    min_similarity = 1\n",
    "    max_key = None\n",
    "    min_key = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        similarity = cosine_similarity(row.values, target)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            max_key = index\n",
    "\n",
    "        if similarity < min_similarity:\n",
    "            min_similarity = similarity\n",
    "            min_key = index\n",
    "\n",
    "    return int(max_key[1:])\n",
    "\n",
    "\n",
    "import heapq\n",
    "\n",
    "def localize_top_k(df, target, k):\n",
    "    similarity_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        similarity = cosine_similarity(row.values, target)\n",
    "        similarity_list.append((similarity, index))\n",
    "\n",
    "    similarity_list.sort(key=lambda x: -x[0])\n",
    "\n",
    "    top_k_similarities = [pair[0] for pair in similarity_list]\n",
    "    top_k_indices = [pair[1] for pair in similarity_list]\n",
    "\n",
    "    return top_k_similarities[:k], top_k_indices[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3977f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "file_path = 'correlate_sensors.yaml'\n",
    "\n",
    "# read YAML file\n",
    "with open(file_path, 'r') as file:\n",
    "    correlate_sensors = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05a83f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use correlate sensor info for localization\n",
    "def localize_ver2(df, target, correlate_sensors):\n",
    "    max_similarity = -1\n",
    "    min_similarity = 1\n",
    "    max_key = None\n",
    "    min_key = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        cur_corr_sensors = correlate_sensors[index]\n",
    "\n",
    "        similarity = cosine_similarity(row[cur_corr_sensors].values, target[cur_corr_sensors])\n",
    "\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            max_key = index\n",
    "\n",
    "        if similarity < min_similarity:\n",
    "            min_similarity = similarity\n",
    "            min_key = index\n",
    "\n",
    "    return int(max_key[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf87a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_folder = './impact_factor/factor_data/'\n",
    "simulate_files = natsorted(os.listdir(simulate_folder))\n",
    "leak_num = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b4a888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone1\n",
      "Leak: p280, Predict: 31, Distance: 50.2, True\n",
      "Leak: p277, Predict: 31, Distance: 414.3, False\n",
      "zone2\n",
      "Leak: p331, Predict: 438, Distance: 104.2, True\n",
      "Leak: p426, Predict: 82, Distance: 226.0, True\n",
      "zone3\n",
      "Leak: p710, Predict: 643, Distance: 172.2, True\n",
      "Leak: p721, Predict: 644, Distance: 299.6, True\n",
      "zone4\n",
      "Leak: p800, Predict: 758, Distance: 174.7, True\n",
      "Leak: p879, Predict: 765, Distance: 0.0, True\n",
      "zone5\n",
      "Leak: p827, Predict: 731, Distance: 0.0, True\n",
      "Leak: p193, Predict: 774, Distance: 122.8, True\n",
      "Leak: p762, Predict: 774, Distance: 946.5, False\n",
      "zone6\n",
      "Leak: p680, Predict: 209, Distance: 74.5, True\n",
      "zone7\n",
      "Leak: p653, Predict: 613, Distance: 197.0, True\n",
      "Leak: p142, Predict: 608, Distance: 292.0, True\n",
      "Leak: p123, Predict: 182, Distance: 296.3, True\n",
      "zone8\n",
      "Leak: p523, Predict: 506, Distance: 66.1, True\n",
      "Leak: p514, Predict: 517, Distance: 0.0, True\n",
      "Leak: p586, Predict: 549, Distance: 170.0, True\n",
      "zone9\n",
      "Leak: p455, Predict: 484, Distance: 244.5, True\n",
      "zone10\n",
      "Localized: 17\n",
      "Avg distance: 202.68\n",
      "Localized: 17, Abrupt: 9, Incipient: 8\n",
      "Avg dist: 202.68, Abrupt: 109.48, Incipient: 286.57\n"
     ]
    }
   ],
   "source": [
    "true_cnt = 0\n",
    "avg_dist = 0\n",
    "use_window_target = True\n",
    "\n",
    "categories_abrupt = ['p280', 'p331', 'p426', 'p710', 'p827', 'p680', 'p142', 'p523', 'p514']\n",
    "categories_incipient = ['p277', 'p721', 'p800', 'p879', 'p193', 'p762', 'p653', 'p586', 'p455', 'p123']\n",
    "dist_abrupt = []\n",
    "dist_incip = []\n",
    "cnt_ab = 0\n",
    "cnt_in = 0\n",
    "\n",
    "for file in simulate_files:\n",
    "    name, _ = os.path.splitext(file)\n",
    "    print(name)\n",
    "    filePath = simulate_folder + file\n",
    "    df = pd.read_csv(filePath,index_col=0)\n",
    "    for detect in detect_dict[name]:\n",
    "        t, pipe = detect\n",
    "        target_vector = sensors_error_df.loc[t]\n",
    "        \n",
    "        if use_window_target:\n",
    "            start_time = str(pd.Timestamp(t) - pd.Timedelta(days=0))\n",
    "            end_time = str(pd.Timestamp(t) + pd.Timedelta(hours=1))\n",
    "\n",
    "#             window_data = df_dict[name].loc[t:end_time]\n",
    "            window_data = sensors_error_df.loc[start_time:end_time]\n",
    "\n",
    "            # 计算时间窗口内的均值\n",
    "            window_mean = window_data.mean()\n",
    "#             target_vector = list(window_mean)\n",
    "            target_vector = window_mean\n",
    "            \n",
    "#         predict = localize(df[sensors_list[name]], target_vector)\n",
    "#         predict = localize(df[sensors_list[name]], target_vector[sensors_list[name]])\n",
    "        predict = localize_ver2(df, target_vector, correlate_sensors)\n",
    "        distance = dist_to_true(predict, pipe)\n",
    "        avg_dist += distance\n",
    "        \n",
    "        cs_max = 0\n",
    "        nei_max = -1\n",
    "        for nei in list(G.neighbors(predict)):\n",
    "            if f'n{nei}' not in df.index:\n",
    "                continue\n",
    "            cs = cosine_similarity((df.loc[f'n{nei}'])[correlate_sensors[f'n{nei}']].values, target_vector[correlate_sensors[f'n{nei}']])\n",
    "            if cs > cs_max:\n",
    "                cs_max = cs\n",
    "                nei_max = nei\n",
    "        predict_pipe = pipe_by_neighbours[str(sorted([predict, nei_max], reverse=True))]\n",
    "        print(f'Leak: {pipe}, Predict: {predict}, Distance: {distance:.1f}, {distance<=300}')\n",
    "        if distance<=300:\n",
    "            true_cnt += 1\n",
    "            \n",
    "            \n",
    "        if pipe in categories_abrupt:\n",
    "            dist_abrupt.append(distance)\n",
    "            if distance<=300:\n",
    "                cnt_ab += 1\n",
    "        else:\n",
    "            dist_incip.append(distance)\n",
    "            if distance<=300:\n",
    "                cnt_in += 1\n",
    "            \n",
    "avg_dist /= leak_num\n",
    "print(f'Localized: {true_cnt}')\n",
    "print(f'Avg distance: {avg_dist:.2f}')\n",
    "avg_ab = sum(dist_abrupt)/len(dist_abrupt)\n",
    "avg_in = sum(dist_incip)/len(dist_incip)\n",
    "print(f'Localized: {true_cnt}, Abrupt: {cnt_ab}, Incipient: {cnt_in}')\n",
    "print(f'Avg dist: {avg_dist:.2f}, Abrupt: {avg_ab:.2f}, Incipient: {avg_in:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
